{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a507b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "training_set=train_datagen.flow_from_directory(\n",
    "    \"Train\",\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cadf08ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2015 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)\n",
    "test_set=test_datagen.flow_from_directory('validation',\n",
    "                                         target_size=(64,64),\n",
    "                                          batch_size=32,\n",
    "                                          class_mode=\"categorical\"\n",
    "                                         \n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb10dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9135967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64,kernel_size=3,activation='relu',input_shape=[64,64,3]))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19889a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))\n",
    "model.add(Conv2D(512,(3,3),activation='relu'))\n",
    "model.add(MaxPool2D(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf941b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13f429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dense(units=128,activation='relu'))\n",
    "model.add(Dense(units=64,activation='relu'))\n",
    "model.add(Dense(units=7,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f212cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "250/250 [==============================] - 307s 1s/step - loss: 1.0426 - accuracy: 0.6666 - val_loss: 0.9499 - val_accuracy: 0.6665\n",
      "Epoch 2/15\n",
      "250/250 [==============================] - 162s 647ms/step - loss: 0.9622 - accuracy: 0.6704 - val_loss: 0.9013 - val_accuracy: 0.6839\n",
      "Epoch 3/15\n",
      "250/250 [==============================] - 115s 461ms/step - loss: 0.9228 - accuracy: 0.6774 - val_loss: 0.9326 - val_accuracy: 0.6868\n",
      "Epoch 4/15\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.9098 - accuracy: 0.6866 - val_loss: 0.8837 - val_accuracy: 0.6784\n",
      "Epoch 5/15\n",
      "250/250 [==============================] - 116s 465ms/step - loss: 0.8925 - accuracy: 0.6794 - val_loss: 0.8472 - val_accuracy: 0.6913\n",
      "Epoch 6/15\n",
      "250/250 [==============================] - 119s 474ms/step - loss: 0.8789 - accuracy: 0.6852 - val_loss: 0.8158 - val_accuracy: 0.6998\n",
      "Epoch 7/15\n",
      "250/250 [==============================] - 117s 468ms/step - loss: 0.8424 - accuracy: 0.6957 - val_loss: 0.8180 - val_accuracy: 0.7002\n",
      "Epoch 8/15\n",
      "250/250 [==============================] - 116s 464ms/step - loss: 0.8386 - accuracy: 0.6923 - val_loss: 0.7895 - val_accuracy: 0.7002\n",
      "Epoch 9/15\n",
      "250/250 [==============================] - 129s 516ms/step - loss: 0.8106 - accuracy: 0.6963 - val_loss: 0.7720 - val_accuracy: 0.7176\n",
      "Epoch 10/15\n",
      "250/250 [==============================] - 127s 505ms/step - loss: 0.8006 - accuracy: 0.7014 - val_loss: 0.7772 - val_accuracy: 0.7132\n",
      "Epoch 11/15\n",
      "250/250 [==============================] - 142s 569ms/step - loss: 0.7905 - accuracy: 0.7113 - val_loss: 0.7902 - val_accuracy: 0.7082\n",
      "Epoch 12/15\n",
      "250/250 [==============================] - 134s 535ms/step - loss: 0.7668 - accuracy: 0.7163 - val_loss: 0.7343 - val_accuracy: 0.7280\n",
      "Epoch 13/15\n",
      "250/250 [==============================] - 135s 539ms/step - loss: 0.7485 - accuracy: 0.7219 - val_loss: 0.7235 - val_accuracy: 0.7390\n",
      "Epoch 14/15\n",
      "250/250 [==============================] - 126s 502ms/step - loss: 0.7529 - accuracy: 0.7221 - val_loss: 0.7253 - val_accuracy: 0.7261\n",
      "Epoch 15/15\n",
      "193/250 [======================>.......] - ETA: 23s - loss: 0.7434 - accuracy: 0.7299"
     ]
    }
   ],
   "source": [
    "model.fit(x=training_set,validation_data=test_set,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7edddb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskin.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"skin.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dceaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"skin.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1abd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 314ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap=cv2.imread(\"E:/Jupyter/datatree/train/akiec/ISIC_0026171.jpg\")\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image=image.load_img('E:/Jupyter/datatree/train/akiec/ISIC_0026171.jpg',target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "result=model.predict(test_image)\n",
    "training_set.class_indices\n",
    "cv2.imshow(\"Image\",cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e84889c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c7419fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Basal cell carcinoma\n"
     ]
    }
   ],
   "source": [
    "if result[0][0]==1:\n",
    "    print(\" This is an Actinic keratoses and intraepithelial carcinoma\")\n",
    "elif result[0][1]==1:\n",
    "    print(\"This is Basal cell carcinoma\")\n",
    "elif result[0][2]:\n",
    "    print(\" This is Benign lesions of the keratosis.\")\n",
    "elif result[0][3]==1:\n",
    "    print(\"This is an Dermatofibroma\")\n",
    "elif result[0][4]==1:\n",
    "    print(\" This is an melanoma\")\n",
    "elif result[0][5]==1:\n",
    "    print(\" This an Nervus Pigmentosus\")\n",
    "elif result[0][6]==1:\n",
    "    print(\" This is an vascular lesions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adbb2318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 64, 64, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name='conv2d_input'), name='conv2d_input', description=\"created by layer 'conv2d_input'\"), but it was called on an input with incompatible shape (32, 64).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"E:\\AC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" (type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (32, 64)\n    \n    Call arguments received by layer \"sequential_1\" (type Sequential):\n      • inputs=tf.Tensor(shape=(32, 64), dtype=uint8)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m roi\u001b[38;5;241m=\u001b[39mimg_to_array(roi)\n\u001b[0;32m     36\u001b[0m roi\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mexpand_dims(roi,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m prediction\u001b[38;5;241m=\u001b[39m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_gray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m label\u001b[38;5;241m=\u001b[39memotion_labels[prediction\u001b[38;5;241m.\u001b[39margmax()]\n\u001b[0;32m     40\u001b[0m label_position\u001b[38;5;241m=\u001b[39m(x,y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mE:\\AC\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqjxtk70g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1845, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1834, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1823, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\training.py\", line 1791, in predict_step\n        return self(x, training=False)\n    File \"E:\\AC\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\AC\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_1\" (type Sequential).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (32, 64)\n    \n    Call arguments received by layer \"sequential_1\" (type Sequential):\n      • inputs=tf.Tensor(shape=(32, 64), dtype=uint8)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Mar 19 20:05:55 2022\n",
    "\n",
    "@author: Toshiba\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep \n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import time\n",
    "import  numpy as np\n",
    "import pyttsx3\n",
    "\n",
    "#face_classifier=cv2.CascadeClassifier(r'E:/Toshiba/Emotion_Detection_CNN/haarcascade_frontalface_default.xml')\n",
    "classifier=load_model('skin.h5')\n",
    "emotion_labels=['Angry','Disgust','Fear','Happy','Neutral','Sad','Surprize']\n",
    "\n",
    "cap=cv2.imread('E:/Jupyter/datatree/train/akiec/ISIC_0026171.jpg')\n",
    "labels=[]\n",
    "#faces=face_classifier.detectMultiScale(gray)\n",
    "\n",
    "x=3\n",
    "y=3 \n",
    "h=3\n",
    "w=2\n",
    "cv2.rectangle(cap, (x,y), (x+w,y+h), (0,255,255),2)\n",
    "roi_gray=gray[y:y+h,x:x+w]\n",
    "roi_gray=cv2.resize(roi_gray, (64,64),interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if np.sum([roi_gray])!=0:\n",
    "    roi=roi_gray.astype('float')/255.0\n",
    "    roi=img_to_array(roi)\n",
    "    roi=np.expand_dims(roi,axis=0)\n",
    "\n",
    "    prediction=classifier.predict(roi_grayray)\n",
    "    label=emotion_labels[prediction.argmax()]\n",
    "    label_position=(x,y-10)\n",
    "    engine=pyttsx3.init('sapi5')\n",
    "    voices=engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[1].id)\n",
    "    def speak(audio):\n",
    "        engine.say(audio)\n",
    "        engine.runAndWait()\n",
    "    def emotion1():\n",
    "        if label==\"Happy\":\n",
    "            speak(f\"Are u in happy face listen some songs \")\n",
    "            time.sleep(0)\n",
    "        if label==\"Sad\":\n",
    "            speak(f\"why are u in sad face go and watch movie\")\n",
    "            time.sleep(0)\n",
    "        if label==\"Neutral\":\n",
    "            speak(f\"listen some Action  songs sir\")\n",
    "            time.sleep(0)\n",
    "\n",
    "\n",
    "    cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0))\n",
    "\n",
    "\n",
    "cv2.imshow(\"Emotion\",cap)\n",
    "#emotion1()\n",
    "#print(label)\n",
    "if cv2.waitKey(1) & 0xFF ==ord('q'):\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfbf01f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '1\\tperson\\t', '2\\tbicycle\\t', '3\\tcar\\t', '4\\tmotorcycle\\t', '5\\tairplane\\t', '6\\tbus\\t', '7\\ttrain\\t', '8\\ttruck\\t', '9\\tboat\\t', '10\\ttraffic light\\t', '11\\tfire hydrant\\t', '12\\tstreet sign', '13\\tstop sign\\t', '14\\tparking meter\\t', '15\\tbench\\t', '16\\tbird\\t', '17\\tcat\\t', '18\\tdog\\t', '19\\thorse\\t', '20\\tsheep\\t', '21\\tcow\\t', '22\\telephant\\t', '23\\tbear\\t', '24\\tzebra\\t', '25\\tgiraffe\\t', '26\\that\\t', '27\\tbackpack\\t', '28\\tumbrella\\t', '29\\tshoe\\t', '30\\teye glasses\\t', '31\\thandbag', '32\\ttie\\t', '33\\tsuitcase\\t', '34\\tfrisbee\\t', '35\\tskis\\t', '36\\tsnowboard\\t', '37\\tsports ball\\t', '38\\tkite\\t', '39\\tbaseball bat', '40\\tbaseball glove\\t', '41\\tskateboard\\t', '42\\tsurfboard\\t', '43\\ttennis racket', '44\\tbottle\\tbottle\\t', '45\\tplate\\t', '46\\twine glass\\t', '47\\tcup\\t', '48\\tfork\\t', '49\\tknife\\t', '50\\tspoon\\t', '51\\tbowl\\t', '52\\tbanana', '53\\tapple\\t', '54\\tsandwich\\t', '55\\torange\\t', '56\\tbroccoli\\t', '57\\tcarrot\\t', '58\\thot dog\\t', '59\\tpizza\\t', '60\\tdonut\\t', '61\\tcake\\t', '62\\tchair\\t', '63\\tcouch\\t', '64\\tpotted plant\\t', '65\\tbed\\t', '66\\tmirror\\t', '67\\tdining', '68\\twindow\\t', '69\\tdesk\\t', '70\\ttoilet\\t', '71\\tdoor\\t', '72\\ttv\\t', '73\\tlaptop', '74\\tmouse\\t', '75\\tremote\\t', '76\\tkeyboard\\t', '77\\tcell phone\\t', '78\\tmicrowave\\t', '79\\toven\\t', '80\\ttoaster\\t', '81\\tsink\\t', '82\\trefrigerator\\t', '83\\tblender\\t', '84\\tbook\\t', '85\\tclock\\t', '86\\tvase\\t', '87\\tscissors\\t', '88\\tteddy bear\\t', '89\\thair drier\\t', '90\\ttoothbrush\\t', '91\\thair brush\\t']\n",
      "[17 67 47 51 51 47 51 59 72 62 51] [[ 23  14 213 135]\n",
      " [  4 102 272  78]\n",
      " [158 138 119  42]\n",
      " [158 138 119  42]\n",
      " [ 10 134 262  46]\n",
      " [179 149  96  32]\n",
      " [178 137  89  23]\n",
      " [  3 134 111  44]\n",
      " [  3   6 203 121]\n",
      " [  3   6 203 121]\n",
      " [179 149  96  32]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep 14 20:11:54 2022\n",
    "\n",
    "@author: Asus\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "img=cv2.imread('E:/Toshiba/rasbbery/cat.jpg')\n",
    "#cap=cv2.VideoCapture(0)\n",
    "#cap.set(3,640)\n",
    "#cap.set(4,480)\n",
    "\n",
    "classNames=[]\n",
    "classFile='E:/Toshiba/rasbbery/coco.txt'\n",
    "with open(classFile,'rt') as f:\n",
    "    classNames=f.read().rstrip('\\n').split('\\n')\n",
    "    print(classNames)\n",
    "configpath='E:/Toshiba/rasbbery/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt.txt'\n",
    "weightspath='E:/Toshiba/rasbbery/frozen_inference_graph.pb'\n",
    "\n",
    "net=cv2.dnn_DetectionModel(weightspath,configpath)\n",
    "net.setInputSize(320,320)\n",
    "net.setInputScale(1.0/127.5)\n",
    "net.setInputMean((127.5,127.5,127.5))\n",
    "net.setInputSwapRB(True)\n",
    "\n",
    "\n",
    "classIds,confs,bbox=net.detect(img,confThreshold=0.25)\n",
    "print(classIds,bbox)\n",
    "for classId,confidence,box in zip(classIds.flatten(),confs.flatten(),bbox) :\n",
    "    cv2.rectangle(img,box,color=(0,255,0),thickness=2)\n",
    "    cv2.putText(img,classNames[classId-1],(box[0]+10,box[1]+30),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "cv2.imshow(\"output\",img)\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0057b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
